{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9af67c15412a4688b2749ced3f492755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0236b27043624cbbb410b30312965543",
              "IPY_MODEL_07022380223d472a8836f0d22649639a",
              "IPY_MODEL_8ebed52d3ad942de85dbaa01736406dd"
            ],
            "layout": "IPY_MODEL_af9b27a8c68f415ab8b27061698d9200"
          }
        },
        "0236b27043624cbbb410b30312965543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_100b9b4b5ed346cabd8422d69950dac5",
            "placeholder": "​",
            "style": "IPY_MODEL_94f014c682ad4eaabf718341d0784c18",
            "value": "Map: 100%"
          }
        },
        "07022380223d472a8836f0d22649639a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c02fdb379b7b41dea845774e35b4376c",
            "max": 1821,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b787cca4e0b4cc7a178751602eb974c",
            "value": 1821
          }
        },
        "8ebed52d3ad942de85dbaa01736406dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19e55d4d60d4dfab22ff3ab9538bbf9",
            "placeholder": "​",
            "style": "IPY_MODEL_4cd55e9595ac47f7ae57c6572e185387",
            "value": " 1821/1821 [00:00&lt;00:00, 3351.10 examples/s]"
          }
        },
        "af9b27a8c68f415ab8b27061698d9200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "100b9b4b5ed346cabd8422d69950dac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f014c682ad4eaabf718341d0784c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c02fdb379b7b41dea845774e35b4376c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b787cca4e0b4cc7a178751602eb974c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c19e55d4d60d4dfab22ff3ab9538bbf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd55e9595ac47f7ae57c6572e185387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alishaw99/GenAI-with-Langchain/blob/main/Amanat_session5_genai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RxTyn1lB-D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.30.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nJd5n4exdK9",
        "outputId": "e3f2d810-2a45-4ea0-8775-73d9ef9d44b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.30.2 in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W43GJrKtCGrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P5cIRzlVB9NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_1YewGDCRKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbunk4Nzj0sK",
        "outputId": "4c7994dd-727d-4f84-ebbc-8d8b7dadac7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hnWhrscCj__F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset=load_dataset('glue','sst2')"
      ],
      "metadata": {
        "id": "KV7VRZfzkJ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'glue':**<br><br>\n",
        "\n",
        "- This is the name of the dataset group. GLUE (General Language Understanding Evaluation) is a benchmark for evaluating the performance of models across various NLP tasks.<br><br>\n",
        "\n",
        "\n",
        "\n",
        "**'sst2':**\n",
        "\n",
        "- This is the name of a specific dataset within the GLUE benchmark. The SST-2 dataset (Stanford Sentiment Treebank) is a collection of movie reviews from Rotten Tomatoes, labeled with sentiment (positive or negative).<br><br>\n",
        "\n",
        "\n",
        "So, when we call load_dataset('glue', 'sst2'), it fetches the SST-2 dataset from the GLUE benchmark. This dataset can then be used for tasks such as sentiment analysis or fine-tuning pre-trained language models"
      ],
      "metadata": {
        "id": "zDmZYUdSj-AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-roQ_W5kWgn",
        "outputId": "24e9a4b9-6372-4fd0-ee38-678ee3aff3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-FbKGRRkZ-H",
        "outputId": "f57d3afa-c13a-43ce-bd1a-9c915d50c77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'label', 'idx'],\n",
              "    num_rows: 67349\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(raw_dataset['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGN5myI5khnN",
        "outputId": "7e04786d-357f-456e-f5ad-e2a4dbc2e508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_TF_DATASET_REFS',\n",
              " '__class__',\n",
              " '__del__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__enter__',\n",
              " '__eq__',\n",
              " '__exit__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getitems__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_build_local_temp_path',\n",
              " '_check_index_is_initialized',\n",
              " '_data',\n",
              " '_estimate_nbytes',\n",
              " '_fingerprint',\n",
              " '_format_columns',\n",
              " '_format_kwargs',\n",
              " '_format_type',\n",
              " '_generate_tables_from_cache_file',\n",
              " '_generate_tables_from_shards',\n",
              " '_get_cache_file_path',\n",
              " '_get_output_signature',\n",
              " '_getitem',\n",
              " '_indexes',\n",
              " '_indices',\n",
              " '_info',\n",
              " '_map_single',\n",
              " '_new_dataset_with_indices',\n",
              " '_output_all_columns',\n",
              " '_push_parquet_shards_to_hub',\n",
              " '_save_to_disk_single',\n",
              " '_select_contiguous',\n",
              " '_select_with_indices_mapping',\n",
              " '_split',\n",
              " 'add_column',\n",
              " 'add_elasticsearch_index',\n",
              " 'add_faiss_index',\n",
              " 'add_faiss_index_from_external_arrays',\n",
              " 'add_item',\n",
              " 'align_labels_with_mapping',\n",
              " 'builder_name',\n",
              " 'cache_files',\n",
              " 'cast',\n",
              " 'cast_column',\n",
              " 'citation',\n",
              " 'class_encode_column',\n",
              " 'cleanup_cache_files',\n",
              " 'column_names',\n",
              " 'config_name',\n",
              " 'data',\n",
              " 'dataset_size',\n",
              " 'description',\n",
              " 'download_checksums',\n",
              " 'download_size',\n",
              " 'drop_index',\n",
              " 'export',\n",
              " 'features',\n",
              " 'filter',\n",
              " 'flatten',\n",
              " 'flatten_indices',\n",
              " 'format',\n",
              " 'formatted_as',\n",
              " 'from_buffer',\n",
              " 'from_csv',\n",
              " 'from_dict',\n",
              " 'from_file',\n",
              " 'from_generator',\n",
              " 'from_json',\n",
              " 'from_list',\n",
              " 'from_pandas',\n",
              " 'from_parquet',\n",
              " 'from_polars',\n",
              " 'from_spark',\n",
              " 'from_sql',\n",
              " 'from_text',\n",
              " 'get_index',\n",
              " 'get_nearest_examples',\n",
              " 'get_nearest_examples_batch',\n",
              " 'homepage',\n",
              " 'info',\n",
              " 'is_index_initialized',\n",
              " 'iter',\n",
              " 'license',\n",
              " 'list_indexes',\n",
              " 'load_elasticsearch_index',\n",
              " 'load_faiss_index',\n",
              " 'load_from_disk',\n",
              " 'map',\n",
              " 'num_columns',\n",
              " 'num_rows',\n",
              " 'prepare_for_task',\n",
              " 'push_to_hub',\n",
              " 'remove_columns',\n",
              " 'rename_column',\n",
              " 'rename_columns',\n",
              " 'reset_format',\n",
              " 'save_faiss_index',\n",
              " 'save_to_disk',\n",
              " 'search',\n",
              " 'search_batch',\n",
              " 'select',\n",
              " 'select_columns',\n",
              " 'set_format',\n",
              " 'set_transform',\n",
              " 'shape',\n",
              " 'shard',\n",
              " 'shuffle',\n",
              " 'size_in_bytes',\n",
              " 'skip',\n",
              " 'sort',\n",
              " 'split',\n",
              " 'supervised_keys',\n",
              " 'take',\n",
              " 'task_templates',\n",
              " 'to_csv',\n",
              " 'to_dict',\n",
              " 'to_iterable_dataset',\n",
              " 'to_json',\n",
              " 'to_list',\n",
              " 'to_pandas',\n",
              " 'to_parquet',\n",
              " 'to_polars',\n",
              " 'to_sql',\n",
              " 'to_tf_dataset',\n",
              " 'train_test_split',\n",
              " 'unique',\n",
              " 'version',\n",
              " 'with_format',\n",
              " 'with_transform']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Calling dir(raw_dataset['train']) will return a list of attributes and methods available for the object corresponding to the training subset of the dataset loaded from the SST-2 dataset."
      ],
      "metadata": {
        "id": "sMj_OUBDksEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(raw_dataset['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "aXSa6N2PkjVA",
        "outputId": "baf95eeb-0214-4c48-92c2-e80e6a6716f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>datasets.arrow_dataset.Dataset</b><br/>def __init__(arrow_table: Table, info: Optional[DatasetInfo]=None, split: Optional[NamedSplit]=None, indices_table: Optional[Table]=None, fingerprint: Optional[str]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py</a>A Dataset backed by an Arrow table.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 668);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- raw_dataset['train'] object belongs to the datasets library's arrow_dataset module, specifically the Dataset class within it."
      ],
      "metadata": {
        "id": "qDZPjeI3k__n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train'].data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg8-5XWikmov",
        "outputId": "583a51ac-a504-4f8f-8063-3156f135a80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MemoryMappedTable\n",
              "sentence: string\n",
              "label: int64\n",
              "idx: int32\n",
              "----\n",
              "sentence: [[\"hide new secretions from the parental units \",\"contains no wit , only labored gags \",\"that loves its characters and communicates something rather beautiful about human nature \",\"remains utterly satisfied to remain the same throughout \",\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \",...,\"you wish you were at home watching that movie instead of in the theater watching this one \",\"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\"underdeveloped \",\"the jokes are flat \",\"a heartening tale of small victories \"],[\"suspense , intriguing characters and bizarre bank robberies , \",\"a gritty police thriller with all the dysfunctional family dynamics one could wish for \",\"with a wonderful ensemble cast of characters that bring the routine day to day struggles of the working class to life \",\"nonetheless appreciates the art and reveals a music scene that transcends culture and race . \",\"do we really need the tiger beat version ? \",...,\"when there 's nothing else happening \",\"on cable \",\"it with ring , \",\"far from a groundbreaking endeavor \",\"that these women are spectacular \"],...,[\"it does turn out to be a bit of a cheat in the end \",\"may be convinced that he has something significant to say \",\"to be both hugely entertaining and uplifting . \",\", boredom never takes hold . \",\"left to work with , sort of like michael jackson 's nose \",...,\"from a severe case of hollywood-itis \",\"the very best of them \",\"thrills , \",\"'s attracting audiences to unfaithful \",\"impressively delicate range \"],[\"starts off promisingly but then proceeds to flop \",\"distinguished actor \",\"on their parents ' anguish \",\"pays off and is effective if you stick with it \",\"is n't particularly funny \",...,\"a delightful comedy \",\"anguish , anger and frustration \",\"at achieving the modest , crowd-pleasing goals it sets for itself \",\"a patient viewer \",\"this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \"]]\n",
              "label: [[0,0,1,0,0,...,0,0,0,0,1],[1,1,1,1,0,...,0,0,1,0,1],...,[0,0,1,1,0,...,0,1,1,1,1],[0,1,0,1,0,...,1,0,1,1,0]]\n",
              "idx: [[0,1,2,3,4,...,995,996,997,998,999],[1000,1001,1002,1003,1004,...,1995,1996,1997,1998,1999],...,[66000,66001,66002,66003,66004,...,66995,66996,66997,66998,66999],[67000,67001,67002,67003,67004,...,67344,67345,67346,67347,67348]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI-5nNEukpNv",
        "outputId": "f2086240-3b0d-475e-9be0-904c71474f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 'hide new secretions from the parental units ',\n",
              " 'label': 0,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train'].features['label'].int2str(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EY2ohoJimWdj",
        "outputId": "954ebf8b-10cd-4082-ca5a-99a42793e0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train'].features['label'].int2str(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x5kZJhrlmdA_",
        "outputId": "533b97fa-1ca4-4399-e959-2fd6dd4dcf3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "XxOarGErlMJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model='distilbert-base-uncased'\n",
        "tokenizer=AutoTokenizer.from_pretrained(model)"
      ],
      "metadata": {
        "id": "KmsH5UkrlS00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84f549e-454d-4009-a8c6-9eb2ea0d5291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The model distilbert-base-uncased refers to a version of the DistilBERT model, which is a distilled and smaller version of the original BERT model. Here's what you can infer about its capabilities:<br><br>\n",
        "\n",
        "**DistilBERT:**\n",
        "\n",
        "- DistilBERT is a transformer-based model developed by Hugging Face. It is trained to understand the context of words in a sentence and can be fine-tuned for various natural language processing (NLP) tasks.<br><br>\n",
        "\n",
        "\n",
        "**Base:**\n",
        "\n",
        "- The term \"base\" typically refers to the size of the model. In this case, it suggests that distilbert-base-uncased is one of the base-sized variants of the DistilBERT model. Base-sized models are smaller and faster compared to larger variants like \"large\" or \"huge,\" but they might sacrifice some performance on certain tasks.<br><br>\n",
        "\n",
        "\n",
        "**Uncased:**\n",
        "\n",
        "- The \"uncased\" suffix indicates that the model is trained on lowercased text. This means that during both pre-training and fine-tuning, the model treats all text as lowercase. This simplifies the model's learning process, especially for languages like English where case may not always be semantically significant."
      ],
      "metadata": {
        "id": "VZdoD5pam9SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sent=tokenizer(raw_dataset['train'][0:3]['sentence'])\n",
        "from pprint import pprint\n",
        "print(tokenized_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMWOvE1hlnPy",
        "outputId": "2ed5e079-2fbb-417b-def1-f1fe2fe66e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102], [101, 3397, 2053, 15966, 1010, 2069, 4450, 2098, 18201, 2015, 102], [101, 2008, 7459, 2049, 3494, 1998, 10639, 2015, 2242, 2738, 3376, 2055, 2529, 3267, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_fn(batch):\n",
        "  return tokenizer(batch['sentence'],truncation=True)"
      ],
      "metadata": {
        "id": "K-W_WMVMl9pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This function takes a batch of sentences, tokenizes them using the specified tokenizer, and returns the tokenized representations, potentially with truncation applied to ensure uniform sequence lengths within the batch.\n",
        "\n",
        "- BERT and DistilBERT models often have a maximum sequence length of 512 tokens.\n",
        "\n",
        "- When truncating sequences, it's common to choose a maximum length that balances computational resources with the need to preserve important information in the text. Choosing a length that is too short may result in loss of information, while choosing a length that is too long may lead to increased computational overhead."
      ],
      "metadata": {
        "id": "RvvxhdLloUNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets=raw_dataset.map(tokenized_fn,batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9af67c15412a4688b2749ced3f492755",
            "0236b27043624cbbb410b30312965543",
            "07022380223d472a8836f0d22649639a",
            "8ebed52d3ad942de85dbaa01736406dd",
            "af9b27a8c68f415ab8b27061698d9200",
            "100b9b4b5ed346cabd8422d69950dac5",
            "94f014c682ad4eaabf718341d0784c18",
            "c02fdb379b7b41dea845774e35b4376c",
            "6b787cca4e0b4cc7a178751602eb974c",
            "c19e55d4d60d4dfab22ff3ab9538bbf9",
            "4cd55e9595ac47f7ae57c6572e185387"
          ]
        },
        "id": "pSsv1w9kmWGO",
        "outputId": "753a0254-68c2-4bf9-d69c-57ae364bdfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9af67c15412a4688b2749ced3f492755"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "pkxY07JlmlO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',           # Output directory\n",
        "#     num_train_epochs=3,               # Number of training epochs\n",
        "#     per_device_train_batch_size=8,    # Batch size per device during training\n",
        "#     per_device_eval_batch_size=8,     # Batch size per device during evaluation\n",
        "#     logging_dir='./logs',             # Directory for storing logs\n",
        "#     logging_steps=100,                # Log every 100 steps\n",
        "#     save_steps=500,                   # Save checkpoint every 500 steps\n",
        "#     evaluation_strategy='epoch',      # Evaluate at the end of each epoch\n",
        "#     save_total_limit=2,               # Limit the total number of checkpoints\n",
        "#     load_best_model_at_end=True,      # Load the best model from checkpoint at the end of training\n",
        "#     metric_for_best_model='accuracy', # Metric to use for saving the best model\n",
        "#     greater_is_better=True            # Whether the 'metric_for_best_model' should be maximized\n",
        "# )"
      ],
      "metadata": {
        "id": "V05b6I8Vqx63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**output_dir:**\n",
        "\n",
        "- This parameter specifies the directory where the model checkpoints and training results will be saved. For example, output_dir='./results' means that all output files will be stored in the results directory.<br><br>\n",
        "\n",
        "\n",
        "**Model Checkpoints:**\n",
        "\n",
        "- Model checkpoints are snapshots of a neural network model's state at certain points during training. These snapshots include the model's architecture as well as the values of its parameters (weights and biases). Checkpoints are typically saved periodically during training to disk, allowing the training process to be resumed from the point at which the checkpoint was saved.<br><br>\n",
        "\n",
        "Checkpoints serve several important purposes:\n",
        "\n",
        "**Resuming Training:** <br><br>\n",
        "\n",
        "If training is interrupted (e.g., due to a system crash or manual termination), checkpoints allow training to be resumed from the most recent saved point, rather than starting from scratch.<br><br>\n",
        "\n",
        "**Evaluation and Inference:**\n",
        "\n",
        "Checkpoints can be loaded for evaluation on a validation or test set, or for making predictions on new data.<br><br>\n",
        "\n",
        "\n",
        "**Model Selection:**\n",
        "\n",
        "Checkpoints can be used to select the best-performing model based on evaluation metrics on a validation set.<br><br>\n",
        "\n",
        "\n",
        "**Training Results:**\n",
        "\n",
        "Training results refer to various information and metrics collected during the training process. These results provide insights into the performance and behavior of the model as it learns from the training data. Common training results include:\n",
        "\n",
        "**Loss Values:**\n",
        "\n",
        "The loss function measures how well the model's predictions match the true labels during training. Monitoring the loss values over time helps assess the model's learning progress.\n",
        "\n",
        "\n",
        "**Metrics:**\n",
        "\n",
        "Metrics such as accuracy, precision, recall, F1-score, etc., are often computed on a validation set during training to evaluate the model's performance. These metrics provide feedback on how well the model generalizes to unseen data.\n",
        "\n",
        "\n",
        "**Learning Curves:**\n",
        "\n",
        "Learning curves visualize the training and validation metrics (e.g., loss, accuracy) over epochs or training steps. They help diagnose issues such as overfitting or underfitting and assess the model's convergence.\n",
        "\n",
        "\n",
        "**Logs:**\n",
        "\n",
        "Logs contain additional information about the training process, such as training time, memory usage, and any warnings or errors encountered."
      ],
      "metadata": {
        "id": "-cHaoA-gq6Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_train_epochs:**\n",
        "\n",
        "This parameter determines the number of training epochs, indicating how many times the model will be trained on the entire training dataset. For example, num_train_epochs=3 means the model will be trained for 3 epochs."
      ],
      "metadata": {
        "id": "Bw6nwrmOsG8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**per_device_train_batch_size:**\n",
        "\n",
        "This parameter sets the batch size per device (e.g., GPU or CPU) during training. For example, per_device_train_batch_size=8 means that 8 samples will be processed in parallel on each device during training."
      ],
      "metadata": {
        "id": "z2lI0KZAsMFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**per_device_eval_batch_size:**\n",
        "\n",
        "Similar to per_device_train_batch_size, this parameter sets the batch size per device during evaluation. For example, per_device_eval_batch_size=8 means that 8 samples will be processed in parallel on each device during evaluation."
      ],
      "metadata": {
        "id": "zkr2u8fosUW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**logging_dir:**\n",
        "\n",
        "This parameter specifies the directory where training logs will be stored. For example, logging_dir='./logs' means that logs will be saved in the logs directory."
      ],
      "metadata": {
        "id": "afp_1EqssaQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**logging_steps:**\n",
        "\n",
        "This parameter determines how often training metrics will be logged, specified as the number of training steps between each log entry. For example, logging_steps=100 means that training metrics will be logged every 100 steps."
      ],
      "metadata": {
        "id": "l8EqjY18sfOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**save_steps:**\n",
        "\n",
        "This parameter sets the frequency of saving model checkpoints during training, specified as the number of training steps. For example, save_steps=500 means that a checkpoint will be saved every 500 steps."
      ],
      "metadata": {
        "id": "h1Qo4SSDsog6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**evaluation_strategy:**\n",
        "\n",
        "\n",
        "This parameter determines when evaluation is performed during training, either at the end of each epoch ('epoch') or at specified intervals of training steps ('steps'). For example, evaluation_strategy='epoch' means that evaluation will be performed at the end of each epoch.<br><br>\n",
        "\n",
        "\n",
        "- evaluation_strategy='epoch' specifies that the model should be evaluated at the end of each training epoch (num_train_epochs=3).<br><br>\n",
        "\n",
        "\n",
        "- After each epoch (full pass through the training dataset), the trainer will automatically evaluate the model on the evaluation dataset (eval_dataset) and compute evaluation metrics (e.g., accuracy, loss).<br><br>\n",
        "\n",
        "\n",
        "- This strategy is useful for monitoring the model's performance over training epochs and detecting any overfitting or underfitting trends."
      ],
      "metadata": {
        "id": "Nea4hzxdszmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**save_total_limit:**\n",
        "\n",
        "This parameter sets a limit on the total number of checkpoints to keep. For example, save_total_limit=2 means that only the two most recent checkpoints will be saved, preventing excessive disk usage."
      ],
      "metadata": {
        "id": "eduODddms7oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load_best_model_at_end:**\n",
        "\n",
        "This parameter specifies whether the best model checkpoint based on the evaluation metric should be loaded at the end of training. For example, load_best_model_at_end=True means that the best model will be loaded."
      ],
      "metadata": {
        "id": "gHG5sVeBtEJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**metric_for_best_model:**\n",
        "\n",
        "This parameter determines the evaluation metric used for selecting the best model checkpoint. For example, metric_for_best_model='accuracy' means that the model with the highest accuracy on the validation set will be selected as the best model."
      ],
      "metadata": {
        "id": "0hq3f1JztJjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**greater_is_better:**\n",
        "\n",
        "\n",
        "This parameter indicates whether a higher value of the evaluation metric is considered better for selecting the best model checkpoint. For example, greater_is_better=True means that higher accuracy values are preferred"
      ],
      "metadata": {
        "id": "ZXwbR6AktRRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accelerate:**<br><br>\n",
        "\n",
        "- Suppose you're training a deep learning model to recognize images. This process involves lots of complex calculations.<br><br>\n",
        "- With accelerate, you can harness the power of specialized hardware like GPUs (Graphics Processing Units) to speed up these calculations.<br><br>\n",
        "- For example, if it takes 10 hours to train your model on a regular computer, accelerate might reduce it to just 2 hours on a GPU.<br><br>\n",
        "- Similarly, when you're running your trained model to make predictions, accelerate helps make those predictions faster, which is crucial for real-time applications like video processing or natural language understanding.<br><br>\n",
        "In summary, bitsandbytes helps with data compression and decompression, while accelerate boosts the performance of deep learning tasks by leveraging specialized hardware."
      ],
      "metadata": {
        "id": "jvBTe_CjtvF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "-VQLhl_C2dCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "MMBgjoVUxpMm",
        "outputId": "12f58ebc-13bf-4802-f91c-0b9e8dd6e978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.21.0\n",
            "  Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.21.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate"
                ]
              },
              "id": "f2511e15263d40ee9664dab9bb805737"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW4uzDDxP6S1",
        "outputId": "85982661-9397-44d9-a212-4d75cfd97956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.12.0,>=1.9->transformers[torch]) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "ATxv8jUqQHrX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "4badd764-4f89-4a72-cef9-5c78e77fad9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.21.0\n",
            "    Uninstalling accelerate-0.21.0:\n",
            "      Successfully uninstalled accelerate-0.21.0\n",
            "Successfully installed accelerate-0.32.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate"
                ]
              },
              "id": "2fc90291ea2940c8b0c3b3ad47db522e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers accelerate\n"
      ],
      "metadata": {
        "id": "F5a8CryORHgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3d8be5-812b-41c4-eff1-a7ab6e79c8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.30.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n",
            "---\n",
            "Name: accelerate\n",
            "Version: 0.32.1\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: zach.mueller@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args=TrainingArguments(\n",
        "    'my_trainer',                          #  Output directory where checkpoints and logs will be saved\n",
        "    evaluation_strategy='epoch',           # Evaluate model at the end of each epoch\n",
        "    save_strategy='epoch',                  # Save model checkpoint at the end of each epoch\n",
        "    num_train_epochs=1,                     # Save model checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "CNeDypZym8Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "3Bk51ZoJqIOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automatic Model Selection:**\n",
        "\n",
        "- The AutoModelForSequenceClassification class automatically selects the appropriate pre-trained model for sequence classification tasks. Sequence classification tasks involve classifying sequences of tokens (such as sentences or documents) into one or more predefined categories."
      ],
      "metadata": {
        "id": "iVeOzydZ4mCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"bert-base-uncased\""
      ],
      "metadata": {
        "id": "0EDz4V645LpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoModelForSequenceClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=2\n",
        ")"
      ],
      "metadata": {
        "id": "rUl6YJKQqP3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102af817-521a-4130-f46e-ac463b9e258b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34YXEvsA_nTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pretrained_model_name_or_path:**\n",
        "\n",
        "This parameter specifies the identifier or path of the pre-trained model to load. It can be the name of a model available in the Hugging Face model hub (e.g., \"bert-base-uncased\") or the path to a directory containing a pre-trained model checkpoint.\n",
        "\n",
        "\n",
        "**num_labels:**\n",
        "\n",
        "\n",
        "This parameter specifies the number of labels or classes in the classification task. For sequence classification tasks, the model's final layer typically has num_labels output units corresponding to the number of classes.\n",
        "\n",
        "\n",
        "**config:**\n",
        "\n",
        "\n",
        "This parameter allows you to pass a model configuration object (PretrainedConfig) to the model. It can be used to customize the model's architecture and behavior.<br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**state_dict:**\n",
        "\n",
        "\n",
        "This parameter allows you to directly initialize the model's parameters (weights and biases) from a PyTorch state_dict. It can be useful for loading custom or modified model weights.\n",
        "cache_dir: This parameter specifies the directory where downloaded model weights will be cached. If not provided, a default cache directory will be used.\n",
        "\n",
        "\n",
        "state_dict Parameter:<br><br>\n",
        "- The state_dict parameter allows you to initialize a model's parameters (weights and biases) directly from a PyTorch state_dict. This is useful when you want to load custom or modified model weights into a pretrained model architecture.\n",
        "\n",
        "\n",
        "        import torch\n",
        "        from transformers import BertModel, BertTokenizer\n",
        "\n",
        "        # Instantiate a pretrained BERT model\n",
        "        model_name = \"bert-base-uncased\"\n",
        "        model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # Suppose you have a custom state_dict with modified weights\n",
        "        custom_state_dict = torch.load(\"custom_bert_state_dict.pth\")\n",
        "\n",
        "        # Load the custom state_dict into the model\n",
        "        model.load_state_dict(custom_state_dict)\n",
        "\n",
        "        # Now `model` has been initialized with the custom weights from `custom_state_dict`\n",
        "\n",
        "\n",
        "cache_dir Parameter:<br><br>\n",
        "\n",
        "- The cache_dir parameter specifies the directory where downloaded model weights will be cached. If this parameter is not provided, a default cache directory will be used to store and retrieve pretrained model weights.\n",
        "\n",
        "\n",
        "      from transformers import BertTokenizer\n",
        "\n",
        "      # Specify a custom cache directory for model weights\n",
        "      custom_cache_dir = \"my_model_cache\"\n",
        "\n",
        "      # Instantiate a BERT tokenizer with a custom cache directory\n",
        "      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=custom_cache_dir)\n",
        "\n",
        "      # Now the model weights downloaded by the tokenizer will be cached in `my_model_cache`\n",
        "\n",
        "\n",
        "**from_tf:**\n",
        "\n",
        "\n",
        "This parameter specifies whether to load the model weights from a TensorFlow checkpoint if available. By default, it's set to False.<br><br>\n",
        "\n",
        "\n",
        "      from transformers import BertModel\n",
        "\n",
        "      # Example: Load a BERT model pretrained using TensorFlow into a PyTorch model\n",
        "      model_name_tf = \"bert-base-uncased-tf\"\n",
        "\n",
        "      # Instantiate a PyTorch model using the TensorFlow checkpoint\n",
        "      model = BertModel.from_pretrained(model_name_tf, from_tf=True)\n",
        "\n",
        "      # Now `model` is initialized with the model weights loaded from a TensorFlow checkpoint\n",
        "\n",
        "\n",
        "**force_download:** <br><br>\n",
        "\n",
        "This parameter specifies whether to force re-download of model weights, even if they already exist in the cache directory. By default, it's set to False.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**resume_download:**\n",
        "\n",
        "\n",
        "This parameter specifies whether to resume interrupted downloads of model weights. By default, it's set to False.\n",
        "local_files_only: This parameter specifies whether to only load model weights from local files and skip downloading from remote sources. By default, it's set to False."
      ],
      "metadata": {
        "id": "5tI4T0qD89k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ZZz8Qj1uqfqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff54840-5cbe-4152-bb19-8563fff3b99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "id": "BhfEIUWrqhsZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "05408b70-464e-4ff8-efea-e1abed50b512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.bert.modeling_bert.BertForSequenceClassification"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.models.bert.modeling_bert.BertForSequenceClassification</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py</a>Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
              "output) e.g. for GLUE tasks.\n",
              "\n",
              "\n",
              "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
              "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
              "etc.)\n",
              "\n",
              "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
              "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
              "and behavior.\n",
              "\n",
              "Parameters:\n",
              "    config ([`BertConfig`]): Model configuration class with all the parameters of the model.\n",
              "        Initializing with a config file does not load the weights associated with the model, only the\n",
              "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1510);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "JUlpx7O9qjd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e04e3cf-9a81-40ce-81c8-4e5c91db15e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "UbMMNZ1-qnZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06077816-61ed-40c9-a0e6-5266cc34a648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================\n",
              "Layer (type:depth-idx)                                  Param #\n",
              "================================================================================\n",
              "BertForSequenceClassification                           --\n",
              "├─BertModel: 1-1                                        --\n",
              "│    └─BertEmbeddings: 2-1                              --\n",
              "│    │    └─Embedding: 3-1                              23,440,896\n",
              "│    │    └─Embedding: 3-2                              393,216\n",
              "│    │    └─Embedding: 3-3                              1,536\n",
              "│    │    └─LayerNorm: 3-4                              1,536\n",
              "│    │    └─Dropout: 3-5                                --\n",
              "│    └─BertEncoder: 2-2                                 --\n",
              "│    │    └─ModuleList: 3-6                             85,054,464\n",
              "│    └─BertPooler: 2-3                                  --\n",
              "│    │    └─Linear: 3-7                                 590,592\n",
              "│    │    └─Tanh: 3-8                                   --\n",
              "├─Dropout: 1-2                                          --\n",
              "├─Linear: 1-3                                           1,538\n",
              "================================================================================\n",
              "Total params: 109,483,778\n",
              "Trainable params: 109,483,778\n",
              "Non-trainable params: 0\n",
              "================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **Input Embeddings:**<br><br>\n",
        "  The input to the model consists of sequences of tokens (words or subwords) from the input text.\n",
        "        Each token is converted into a vector representation using an embedding layer.\n",
        "        \n",
        "  There are three types of embeddings:\n",
        "\n",
        "\n",
        "        Word Embeddings:\n",
        "        \n",
        "        Each token is mapped to a vector representation using a pre-trained embedding matrix. This captures the meaning of individual words.\n",
        "\n",
        "\n",
        "        Position Embeddings:\n",
        "        \n",
        "        Each token is assigned a position embedding vector, indicating its position in the sequence. This helps the model understand the order of tokens in the input.\n",
        "\n",
        "        Token Type Embeddings:\n",
        "        \n",
        "        For tasks involving multiple sequences (e.g., question answering), each sequence is assigned a token type embedding to distinguish between them.\n",
        "\n",
        "\n",
        "        Encoder Layers:\n",
        "        The model consists of multiple layers of transformer blocks, called\n",
        "        BertLayers.\n",
        "\n",
        "\n",
        "        Each BertLayer contains two main components:\n",
        "\n",
        "        Self-Attention Mechanism:\n",
        "        \n",
        "        This mechanism allows the model to weigh the importance of each token in the sequence based on its relevance to other tokens. It attends to different parts of the input sequence when generating representations.\n",
        "        Feedforward Neural Networks: After attending to relevant parts of the input sequence, the model passes the attended representations through feedforward neural networks to capture complex patterns and relationships in the data.\n",
        "\n",
        "\n",
        "        Pooling Layer:\n",
        "\n",
        "\n",
        "        After processing all the tokens in the sequence through the encoder layers, the model applies a pooling mechanism to aggregate the token-level representations into a single representation for the entire sequence.\n",
        "        In this case, the pooling mechanism uses a fully connected layer followed by a non-linear activation function (Tanh) to compute the pooled representation.\n",
        "\n",
        "\n",
        "        Output Layer:\n",
        "\n",
        "        The pooled representation is passed through a linear layer (classifier) to produce the final output.\n",
        "        For sequence classification tasks, such as sentiment analysis or text categorization, the output layer has two units (out_features=2), representing the probabilities of belonging to each class (e.g., positive or negative sentiment).\n",
        "\n",
        "\n",
        "        Dropout:\n",
        "\n",
        "        \n",
        "        Dropout is applied throughout the model to prevent overfitting by randomly setting a fraction of input units to zero during training. This helps the model generalize better to unseen data."
      ],
      "metadata": {
        "id": "pj4CPHR06XJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_before=[]\n",
        "for name,p in model.named_parameters():\n",
        "  params_before.append(p.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "iBD0QRflqxzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After executing this code, params_before will contain the numpy representations of all the model parameters. These representations can be used for various purposes such as comparison, visualization, or tracking parameter changes during training."
      ],
      "metadata": {
        "id": "QF3rckDO7fXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer"
      ],
      "metadata": {
        "id": "39X0yjFUrMh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "k79oq8JRrQxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric=load_metric('glue','sst2')"
      ],
      "metadata": {
        "id": "xB0cHwL4rY0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d73ebe-e5f7-4a39-9a87-bb48487761bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-70c9b2e53933>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric=load_metric('glue','sst2')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/glue.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric.compute(predictions=[1,0,1],references=[1,0,0])"
      ],
      "metadata": {
        "id": "89PYzJzarf13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c3b35c-e146-4c2c-badb-4031334bf5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.6666666666666666}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(logits_and_labels):\n",
        "  logits,labels=logits_and_labels\n",
        "  predictions=np.argmax(logits,axis=1)\n",
        "  return metric.compute(predictions=predictions,references=labels)"
      ],
      "metadata": {
        "id": "n2K23U2zruHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "YFy5wSQtsQVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "vbZoZ4_Xs1xw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "9bb5193a-68f2-4544-d515-1a571aac307e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8419' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8419/8419 12:11, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.203000</td>\n",
              "      <td>0.329301</td>\n",
              "      <td>0.910550</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8419, training_loss=0.2696503252835267, metrics={'train_runtime': 734.6477, 'train_samples_per_second': 91.675, 'train_steps_per_second': 11.46, 'total_flos': 1029664559600160.0, 'train_loss': 0.2696503252835267, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5ufQMjb_Nsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**global_step:**\n",
        "\n",
        "\n",
        "\n",
        "- This represents the total number of optimization steps (parameter updates) performed during training. Each optimization step corresponds to one batch of training data processed.\n",
        "\n",
        "\n",
        "\n",
        "**training_loss:**\n",
        "\n",
        "\n",
        "\n",
        "- This is the average loss value computed during the training process. It represents how well the model's predictions match the ground truth labels on the training data. Lower values indicate better performance.\n",
        "\n",
        "\n",
        "\n",
        "**metrics:**\n",
        "\n",
        "\n",
        "\n",
        "- This dictionary contains additional metrics and information related to the training process. Let's break down each metric:\n",
        "\n",
        "\n",
        "- **train_runtime:**\n",
        "\n",
        "\n",
        "- This is the total runtime of the training process in seconds. It represents the amount of time taken to complete the training.\n",
        "\n",
        "\n",
        "- **train_samples_per_second:**\n",
        "\n",
        "\n",
        "- This metric indicates the number of training samples processed per second. It represents the training speed or throughput of the model.\n",
        "\n",
        "\n",
        "- **train_steps_per_second:**\n",
        "\n",
        "\n",
        "- This metric indicates the number of training steps (parameter updates) performed per second. It represents the training speed in terms of optimization steps.\n",
        "\n",
        "\n",
        "**total_flos:**\n",
        "\n",
        "\n",
        "\n",
        "- This metric represents the total number of floating-point operations (FLOPs) executed during training. FLOPs are a measure of computational complexity and represent the total amount of arithmetic operations performed by the model.\n",
        "\n",
        "\n",
        "**train_loss:**\n",
        "\n",
        "\n",
        "\n",
        "- This is the same as training_loss. It represents the average loss value computed during training.\n",
        "\n",
        "\n",
        "**epoch:**\n",
        "\n",
        "\n",
        "- This indicates the current epoch of training. In your case, it shows 1.0, which means that one epoch of training has been completed."
      ],
      "metadata": {
        "id": "YLsrnWHP-cwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('my_model')"
      ],
      "metadata": {
        "id": "ezBPd5DGs44s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "mFh3SrmAtDIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model=pipeline('text-classification',model='my_model',device=0)"
      ],
      "metadata": {
        "id": "Q906pxkNtIOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4595705-c45f-479c-b871-e6c13574f38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model('this movie is so so')"
      ],
      "metadata": {
        "id": "5MGratJZtWp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45aa4ccd-160b-486f-a11d-8776249f11bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_1', 'score': 0.9734802842140198}]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training large models like LLaMA2, Zephyr, or Mistral on Google Colab's GPU can be challenging due to the limited resources and memory constraints. Here's a brief analysis of each model's requirements:<br><br>\n",
        "\n",
        "**LLaMA2:**  \n",
        "\n",
        "This model has approximately 1.4 billion parameters, which requires a significant amount of memory and computational resources.<br><br>\n",
        "\n",
        "\n",
        "**Zephyr:**\n",
        "\n",
        "Zephyr is a large language model with around 2.5 billion parameters, making it even more resource-intensive than LLaMA2.<br><br>\n",
        "\n",
        "**Mistral:**\n",
        "\n",
        "Mistral is a massive model with around 5 billion parameters, which is one of the largest language models available.<br><br>\n",
        "\n",
        "\n",
        "Google Colab's GPU resources are limited, and training these models might not be feasible or would require significant modifications to the training script. Here are some limitations to consider:<br><br>\n",
        "\n",
        "**GPU Memory:**\n",
        "\n",
        "Colab's GPU has 16 GB of VRAM, which might not be enough to hold the model's parameters and the input data.<br><br>\n",
        "\n",
        "\n",
        "**Compute Resources:**\n",
        "\n",
        "Colab's GPU has a limited number of CUDA cores and memory bandwidth, which can slow down the training process.<br><br>\n",
        "\n",
        "If you still want to try training these models on Colab, here are some possible workarounds:<br><br>\n",
        "\n",
        "**Model parallelism:**\n",
        "\n",
        "You can try to split the model across multiple GPUs using model parallelism techniques, such as torch.distributed or transformers' built-in parallelism features. This would require significant modifications to the training script.<br><br>\n",
        "\n",
        "**Gradient checkpointing:**\n",
        "\n",
        "You can use gradient checkpointing to reduce the memory requirements during training. This technique saves only the gradients of the model's parameters at certain intervals, reducing the memory footprint.<br><br>\n",
        "\n",
        "\n",
        "**Mixed precision training:**\n",
        "\n",
        "You can use mixed precision training, which uses lower precision data types (e.g., float16) for the model's parameters and activations, reducing the memory requirements.\n",
        "However, even with these workarounds, training these large models on Colab's GPU might not be feasible or would require an impractically long time.\n",
        "\n",
        "Estimated training time on Colab's GPU:\n",
        "\n",
        "Assuming you can modify the training script to accommodate the model's size, here are rough estimates of the training time on Colab's GPU:\n",
        "\n",
        "      LLaMA2: 1-2 weeks (depending on the batch size and sequence length)\n",
        "      Zephyr: 2-4 weeks (depending on the batch size and sequence length)\n",
        "      Mistral: 4-8 weeks (depending on the batch size and sequence length)\n",
        "\n",
        "\n",
        "Keep in mind that these estimates are rough and might not be accurate, as the training time depends on many factors, including the model's architecture, dataset size, batch size, and sequence length.<br><br>\n",
        "\n",
        "**Alternate options:**\n",
        "\n",
        "If you cannot train these models on Colab's GPU, consider the following alternatives:\n",
        "\n",
        "**Cloud services:**\n",
        "- Use cloud services like AWS, Google Cloud, or Microsoft Azure, which offer more powerful GPU instances with larger memory and compute resources.<br><br>\n",
        "\n",
        "\n",
        "**Local machine:**\n",
        "\n",
        "- If you have a powerful local machine with a high-end GPU (e.g., NVIDIA V100 or A100), you can train the models locally.<br><br>\n",
        "\n",
        "\n",
        "**Distributed training:**\n",
        "\n",
        "- Use distributed training frameworks like transformers' Trainer with DistributedDataParallel or torch.distributed to train the models on multiple machines or GPUs.<br><br>\n",
        "\n",
        "**Pre-trained models:**\n",
        "\n",
        "- If you don't need to fine-tune the models from scratch, you can use pre-trained models available on the Hugging Face model hub, which can save you a significant amount of time and resources."
      ],
      "metadata": {
        "id": "95O_WLgZiGL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning:**<br><br>\n",
        "\n",
        "\n",
        "- In transfer learning, a pre-trained model (often trained on a large dataset) is used as a starting point for a new task. The weights of the pre-trained model are initialized based on its previous training.\n",
        "During transfer learning, the model is typically adapted to the new task by adjusting its weights using a smaller learning rate than during the initial training. This allows the model to fine-tune its parameters to better suit the characteristics of the new task.<br><br>\n",
        "- The degree of adjustment to the weights in transfer learning may vary depending on the similarity between the original task and the new task. If the tasks are closely related, the weights may require only minor adjustments to perform well on the new task.<br><br>\n",
        "\n",
        "\n",
        "**Fine-Tuning:**<br><br>\n",
        "\n",
        "\n",
        "- Fine-tuning is a specific form of transfer learning where the pre-trained model's parameters are further adjusted (fine-tuned) on a new dataset or task.<br><br>\n",
        "\n",
        "- In fine-tuning, the weights of the pre-trained model are updated more extensively compared to transfer learning. The entire model (or a portion of it) is trained on the new dataset, and the learning rate used for updating the weights may be smaller than during the initial training but larger than in transfer learning.<br><br>\n",
        "\n",
        "\n",
        "- Fine-tuning allows the model to adapt more closely to the specifics of the new task, potentially resulting in better performance on the task compared to transfer learning alone.<br><br>\n",
        "\n",
        "\n",
        "In summary, while both transfer learning and fine-tuning involve adjusting the weights of a pre-trained model for a new task, fine-tuning typically involves more extensive weight adjustments and training on the new dataset. Transfer learning may involve less extensive adjustments, but it still adapts the pre-trained model to the characteristics of the new task to some degree."
      ],
      "metadata": {
        "id": "Ls11aU47Qg8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is the difference between fine-tuning and parameter-efficient fine-tuning?"
      ],
      "metadata": {
        "id": "4C1zvXQ-RlEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fine-tuning and parameter-efficient fine-tuning are two approaches used in machine learning to improve the performance of pre-trained models on a specific task.<br><br>\n",
        "\n",
        "- Fine-tuning is taking a pre-trained model and training it further on a new task with new data. The entire pre-trained model is usually trained in fine-tuning, including all its layers and parameters. This process can be computationally expensive and time-consuming, especially for large models.<br><br>\n",
        "\n",
        "- On the other hand, parameter-efficient fine-tuning is a method of fine-tuning that focuses on training only a subset of the pre-trained model’s parameters. This approach involves identifying the most important parameters for the new task and only updating those parameters during training. Doing so, PEFT can significantly reduce the computation required for fine-tuning."
      ],
      "metadata": {
        "id": "VmbpWeQBRrh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "def display_image(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        display(img)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)"
      ],
      "metadata": {
        "id": "8VlVX4N_Sd17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image('/content/finetuning.jpg')"
      ],
      "metadata": {
        "id": "AGMO18OlSR8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd38f201-a433-4c42-f040-7512d91bc70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: [Errno 2] No such file or directory: '/content/finetuning.jpg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter-efficient fine-tuning techniques"
      ],
      "metadata": {
        "id": "Gv4SXvCETF4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adapter**<br><br>\n",
        "\n",
        "- Adapters are a special type of submodule that can be added to pre-trained language models to modify their hidden representation during fine-tuning. By inserting adapters after the multi-head attention and feed-forward layers in the transformer architecture, we can update only the parameters in the adapters during fine-tuning while keeping the rest of the model parameters frozen.\n",
        "\n",
        "- Adopting adapters can be a straightforward process. All that is required is to add adapters into each transformer layer and place a classifier layer on top of the pre-trained model. By updating the parameters of the adapters and the classifier head, we can improve the performance of the pre-trained model on a particular task without updating the entire model. This approach can save time and computational resources while still producing impressive results."
      ],
      "metadata": {
        "id": "FA_b2eI4TN4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LoRA**<br><br>\n",
        "\n",
        "\n",
        "- Low-Rank Adaptation (LoRA) of large language models is another approach in the area of fine-tuning models for specific tasks or domains. Similar to the adapters, LoRA is also a small trainable submodule that can be inserted into the transformer architecture. It involves freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the transformer architecture, greatly diminishing the number of trainable parameters for downstream tasks. This method can minimize the number of trainable parameters by up to 10,000 times and the GPU memory necessity by 3 times while still performing on par or better than fine-tuning model quality on various tasks. LoRA also allows for more efficient task-switching, lowering the hardware barrier to entry, and has no additional inference latency compared to other methods."
      ],
      "metadata": {
        "id": "pEaM0pfaTX3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tune Llama 3 with ORPO"
      ],
      "metadata": {
        "id": "uLHnhv6RGT_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ORPO is a new exciting fine-tuning technique that combines the traditional supervised fine-tuning and preference alignment stages into a single process. This reduces the computational resources and time required for training. Moreover, empirical results demonstrate that ORPO outperforms other alignment methods on various model sizes and benchmarks."
      ],
      "metadata": {
        "id": "_AvpZoP_GmYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are now many methods to align large language models (LLMs) with human preferences. Reinforcement learning with human feedback (RLHF) was one of the first and brought us ChatGPT, but RLHF is very costly. DPO**(Differentiable Preference Optimization)**, IPO**(Interactive Preference Optimization)**, and KTO **(Knowledge Transfer Optimization)** are notably cheaper than RLHF as they don’t need a reward model.<br><br>\n",
        "\n",
        "- While DPO and IPO are cheaper, they still require to train two different models. One model for the supervised fine-tuning (SFT) step, i.e., training the model to answer instructions, and then the model to align with human preferences using the SFT model for initialization and as a reference.<br><br>\n",
        "\n",
        "- ORPO is yet another new method for LLM alignment but this one doesn’t even need the SFT model. With ORPO, the LLM jointly learns to answer instructions and human preferences."
      ],
      "metadata": {
        "id": "LmMJ88NHHRWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)"
      ],
      "metadata": {
        "id": "j41OFcG6IJJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ORPO\n",
        "\n",
        "Instruction tuning and preference alignment are essential techniques for adapting Large Language Models (LLMs) to specific tasks. Traditionally, this involves a multi-stage process: 1/ Supervised Fine-Tuning (SFT) on instructions to adapt the model to the target domain, followed by 2/ preference alignment methods like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to increase the likelihood of generating preferred responses over rejected ones."
      ],
      "metadata": {
        "id": "zgizHaw2JNjD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvRVDA8BJZNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image('/content/ftrth4Q.png')"
      ],
      "metadata": {
        "id": "apBHTdpnJsXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ce2442-e488-49a0-8757-08be0511b0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: [Errno 2] No such file or directory: '/content/ftrth4Q.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, researchers have identified a limitation in this approach. While SFT effectively adapts the model to the desired domain, it inadvertently increases the probability of generating undesirable answers alongside preferred ones. This is why the preference alignment stage is necessary to widen the gap between the likelihoods of preferred and rejected outputs."
      ],
      "metadata": {
        "id": "GlyjVj_qJ24T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_image('/content/zWnTNlH.png')"
      ],
      "metadata": {
        "id": "Ns3dRp8QKD0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f10deb-4a5f-43c3-ffc6-59e0f1a5c31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: [Errno 2] No such file or directory: '/content/zWnTNlH.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduced by Hong and Lee (2024), ORPO offers an elegant solution to this problem by combining instruction tuning and preference alignment into a single, monolithic training process."
      ],
      "metadata": {
        "id": "4jcX1RP6KPRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_image('/content/IFeK7DO.png')"
      ],
      "metadata": {
        "id": "Bx83S72PKzpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab3df21-83e4-45d9-ca6c-a0cda2e0e422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: [Errno 2] No such file or directory: '/content/IFeK7DO.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets accelerate peft trl bitsandbytes wandb\n"
      ],
      "metadata": {
        "id": "y08Ea7IeLCKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfcf80d0-dcf0-4772-f886-d008b2ebe522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Collecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.10.0-py2.py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.1/302.1 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Installing collected packages: smmap, shtab, setproctitle, sentry-sdk, docker-pycreds, gitdb, tyro, tokenizers, gitpython, wandb, transformers, bitsandbytes, trl, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.30.2\n",
            "    Uninstalling transformers-4.30.2:\n",
            "      Successfully uninstalled transformers-4.30.2\n",
            "Successfully installed bitsandbytes-0.43.1 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 peft-0.11.1 sentry-sdk-2.10.0 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 tokenizers-0.19.1 transformers-4.42.4 trl-0.9.6 tyro-0.8.5 wandb-0.17.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "15b8e954ad9f420c8ed40ebe905a8e90"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Accelerate**\n",
        "\n",
        "\n",
        "\n",
        "Accelerate is a library that provides a set of tools to accelerate the training of deep learning models. It's designed to work seamlessly with popular deep learning frameworks like PyTorch and TensorFlow.\n",
        "\n",
        "Accelerate provides several features to speed up training, including:\n",
        "\n",
        "**Mixed precision training:**\n",
        "\n",
        "- Accelerate allows you to use lower precision data types (e.g., float16) for model weights and activations, which can significantly reduce memory usage and improve training speed.<br><br>\n",
        "\n",
        "\n",
        "**Gradient checkpointing:**\n",
        "\n",
        "- Accelerate provides a gradient checkpointing mechanism that allows you to store only the gradients of the model's parameters at certain intervals, reducing memory usage and improving training speed.<br><br>\n",
        "\n",
        "\n",
        "**Distributed training:**\n",
        "\n",
        "- Accelerate provides a distributed training framework that allows you to scale your training process across multiple machines, making it ideal for large-scale deep learning models"
      ],
      "metadata": {
        "id": "jE9VZBwGMLx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Wandb:**<br><br>\n",
        "\n",
        "- The Weights & Biases library (wandb) is a tool for visualizing and tracking machine learning experiments. It provides features for logging model training metrics, visualizing training progress with interactive charts, and comparing experiments across different runs. It's commonly used by researchers and practitioners to monitor and analyze the performance of their machine learning models."
      ],
      "metadata": {
        "id": "kVPOARbET2_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TRL - Transformer Reinforcement Learning**\n",
        "\n",
        "- TRL is a full stack library where we provide a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. <br><br>\n",
        "\n",
        "\n",
        "\n",
        "**Model Classes:**\n",
        "\n",
        "A brief overview of what each public model class does.<br><br>\n",
        "\n",
        "\n",
        "**SFTTrainer:**\n",
        "\n",
        "Supervise Fine-tune your model easily with SFTTrainer<br><br>\n",
        "\n",
        "\n",
        "**RewardTrainer:**\n",
        "\n",
        "Train easily your reward model using RewardTrainer.<br><br>\n",
        "\n",
        "\n",
        "**PPOTrainer:**\n",
        "\n",
        "Further fine-tune the supervised fine-tuned model using PPO algorithm\n",
        "Best-of-N Sampling: Use best of n sampling as an alternative way to sample predictions from your active model<br><br>\n",
        "\n",
        "\n",
        "**DPOTrainer:**\n",
        "\n",
        "\n",
        "Direct Preference Optimization training using DPOTrainer.\n",
        "TextEnvironment: Text environment to train your model using tools with RL."
      ],
      "metadata": {
        "id": "Jmzum4xQUXBN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WgGYcSV3Uz-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**BitsAndBytes**"
      ],
      "metadata": {
        "id": "dlYFcYpaXS61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizers:**<br><br>\n",
        "\n",
        "\n",
        "- Optimizers are algorithms used to adjust the parameters of a machine learning model during training in order to minimize the error or loss function. These algorithms play a crucial role in the training process by determining how the model's parameters are updated based on the gradients of the loss function with respect to those parameters.<br><br>\n",
        "\n",
        "- Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, among others. Each optimizer has its own update rules and hyperparameters that influence the training dynamics and convergence properties of the model.<br><br>\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose you're training a neural network for image classification. During each training iteration, the optimizer computes the gradients of the loss function with respect to the model's parameters and updates the parameters accordingly to minimize the classification error."
      ],
      "metadata": {
        "id": "zVGKupdsXW-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision:**<br><br>\n",
        "\n",
        "\n",
        "- Precision refers to the level of numerical accuracy or representation used to store and process data in a computational system. In the context of machine learning and optimization, precision often refers to the number of bits used to represent numerical values, particularly floating-point numbers.<br><br>\n",
        "\n",
        "- Higher precision allows for greater numerical accuracy but requires more memory and computational resources. Conversely, lower precision reduces memory usage and computational overhead but may introduce quantization errors or numerical instability.<br><br>\n",
        "\n",
        "\n",
        "- Precision is commonly expressed in terms of the number of bits used to represent numerical values, such as 16-bit (half precision), 32-bit (single precision), or 64-bit (double precision).<br><br>\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "In training neural networks, the weights, biases, and gradients are typically represented using 32-bit floating-point numbers (float32) for high precision. However, for memory-constrained environments or specialized hardware accelerators, lower precision formats like 16-bit floating-point numbers (float16) or even 8-bit integers (int8) may be used to reduce memory consumption and improve computational efficiency while still achieving acceptable performance."
      ],
      "metadata": {
        "id": "4muqdwNBX2fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**BitsAndBytes**"
      ],
      "metadata": {
        "id": "ohOFQyldY43Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The bitsandbytes library provides functionalities for reducing memory consumption in large language models (LLMs) using k-bit quantization techniques. <br><br>\n",
        "\n",
        "Let's break down each feature and explain them with examples:<br><br>\n",
        "\n",
        "**8-bit Optimizers:**<br><br>\n",
        "\n",
        "\n",
        "- This feature utilizes block-wise quantization to maintain 32-bit performance while significantly reducing memory consumption. It optimizes the memory usage of the optimizer during training.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose you're training a large language model using PyTorch with a 32-bit optimizer. By using bitsandbytes's 8-bit optimizer feature, you can quantize the optimizer to use only 8 bits for certain operations, dramatically reducing memory usage without sacrificing performance.<br><br>\n",
        "\n",
        "\n",
        "**LLM.Int() or 8-bit Quantization for Inference:**<br><br>\n",
        "\n",
        "\n",
        "- This feature enables large language model inference with only half the required memory and without any performance degradation. It achieves this by quantizing most features to 8 bits and separately treating outliers with 16-bit matrix multiplication.\n",
        "\n",
        "Example:\n",
        "\n",
        "You have a pre-trained language model like BERT, which requires significant memory for inference due to its large size. By applying bitsandbytes's LLM.Int() feature, you can quantize the model's parameters to 8 bits, reducing the memory required for inference while maintaining performance.<br><br>\n",
        "\n",
        "\n",
        "**QLoRA or 4-bit Quantization for Training:**<br><br>\n",
        "\n",
        "\n",
        "- This feature enables large language model training with memory-saving techniques that do not compromise performance. It quantizes the model to 4 bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allow training.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "You want to fine-tune a large language model on a specific task, but memory constraints make it challenging. By using bitsandbytes's QLoRA feature, you can quantize the model to 4 bits and introduce LoRA weights, reducing memory usage during training without sacrificing performance."
      ],
      "metadata": {
        "id": "vxER59qSYvfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from google.colab import userdata\n",
        "from peft.tuners import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)\n"
      ],
      "metadata": {
        "id": "Dbp9YLUtZ8Ay",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "8a72d08a-7bb5-4817-ce5f-da7b56c97b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'PeftModel' from 'peft.tuners' (/usr/local/lib/python3.10/dist-packages/peft/tuners/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-e5b4b3cc831b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuners\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from transformers import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PeftModel' from 'peft.tuners' (/usr/local/lib/python3.10/dist-packages/peft/tuners/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install -qqq flash-attn\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "    torch_dtype = torch.bfloat16\n",
        "else:\n",
        "    attn_implementation = \"eager\"\n",
        "    torch_dtype = torch.float16\n"
      ],
      "metadata": {
        "id": "ChevT-dtaNEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The condition torch.cuda.get_device_capability()[0] >= 8 checks if the CUDA compute capability of the GPU is greater than or equal to 8.<br><br>\n",
        "\n",
        "- CUDA capability 8 refers to GPUs with the Ampere architecture, which introduced significant improvements in performance, efficiency, and feature support compared to previous architectures."
      ],
      "metadata": {
        "id": "NFMb13Jhb279"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **torch.float16** represents 16-bit floating-point numbers (half precision), which offer lower precision but require less memory compared to 32-bit floating-point numbers (float32).<br><br>\n",
        "\n",
        "\n",
        "- **torch.bfloat16** represents 16-bit floating-point numbers using the bfloat16 format, which provides greater precision than float16 for certain operations while still offering memory savings compared to float32."
      ],
      "metadata": {
        "id": "nnxdhSs4cCfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, we will load the Llama 3 8B model in 4-bit precision thanks to bitsandbytes. We then set the LoRA configuration using PEFT for QLoRA. I'm also using the convenient setup_chat_format() function to modify the model and tokenizer for ChatML support. It automatically applies this chat template, adds special tokens, and resizes the model's embedding layer to match the new vocabulary size."
      ],
      "metadata": {
        "id": "rZCQp1j3cYiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
        "new_model = \"OrpoLlama-3-8B\"\n",
        "\n",
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=attn_implementation\n",
        ")\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ],
      "metadata": {
        "id": "U55DLNV0cgZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Link to understand Quantization](https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2)"
      ],
      "metadata": {
        "id": "EtcjkTTDeVfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bnb_4bit_quant_type:**\n",
        "\n",
        "- NF4 (NormalFloat) is a 4-bit data type used in machine learning, which normalizes each weight to a value between -1 and 1 for a more accurate representation of the lower precision weight. It is an enhancement of the Quantile Quantization technique and has shown better results than both 4-bit Integers and 4-bit Floats. NF4 can also be coupled with Double-Quantization (DQ) for higher compression while maintaining performance.<br><br>\n",
        "\n",
        "**bnb_4bit_use_double_quant**\n",
        "\n",
        "\n",
        "- DQ encompasses two quantization phases; initially, quantization constants are processed, which are then used as inputs for the subsequent quantization, yielding FP32 and FP8 values. This method avoids any performance drop, while saving an average of about 0.37 bits per parameter (approximately 3 GB for a 65B model). The recent integration of bitsandbytes, which incorporates findings from the QLoRA paper (including NF4 and DQ), shows virtually no reduction in performance with 4-bit quantization for both inferring and training large language models. NF4 and Double Quantization can be leveraged using the bitsandbytes library which is integrated inside the transformers library. Here is an example of how to easily load and quantize any Hugging Face model using the bitsandbytes library:"
      ],
      "metadata": {
        "id": "qcS8j8iifUas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bnb_4bit_compute_dtype:**<br><br>\n",
        "\n",
        "- This parameter specifies the data type to be used for computation during training or inference with 4-bit quantized parameters. It is set based on the conditional logic in the previous code snippet, where torch_dtype is determined.\n",
        "In the provided example, torch_dtype could be either torch.bfloat16 or torch.float16, depending on the CUDA device capability."
      ],
      "metadata": {
        "id": "EK6U4ku2dBN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LoraConfig class is used to configure the LoRa (Layer-wise Relevance Analysis) technique for fine-tuning large language models. <br><br>\n",
        "\n",
        "The parameters in the peft_config object are as follows:<br><br>\n",
        "\n",
        "\n",
        "**r:**\n",
        "\n",
        "- The r parameter specifies the rank of the LoRA projection matrix. This parameter controls the number of parameters used for fine-tuning. A higher rank leads to more parameters and potentially better performance, but it also increases the memory footprint of the model<br><br>\n",
        "\n",
        "\n",
        "**lora_alpha:**\n",
        "\n",
        "- The lora_alpha parameter controls the scale of the LoRA projection matrix.\n",
        "You can use this parameter to adjust the learning rate for the fine-tuning process. For optimal results, set it to double the value of r.<br><br>\n",
        "\n",
        "\n",
        "**lora_dropout:**\n",
        "\n",
        "-  The lora_dropout parameter specifies the dropout rate for the LoRA projection matrix.\n",
        "This parameter can help prevent overfitting and enhance the model’s generalization ability.<br><br>\n",
        "\n",
        "\n",
        "**bias:**\n",
        "\n",
        "- The bias parameter specifies whether to use a bias term in the LoRA projection matrix. Setting this to “none” implies that no bias term will be used.<br><br>\n",
        "\n",
        "\n",
        "**task_type:**\n",
        "\n",
        "- The task_type parameter specifies the type of task the model will be used for. In this case, the task type is set to “CAUSAL_LM”, indicating that the model will be used for causal language modeling.<br><br>\n",
        "\n",
        "**target_modules:**\n",
        "\n",
        "-  The list of modules in the model to apply the LoRa technique to. This parameter specifies the modules where the adapter matrices will be added."
      ],
      "metadata": {
        "id": "bGYb5Ov8gfPl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-R_PgH5k6ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
        "dataset = load_dataset(dataset_name, split=\"all\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(10))\n",
        "\n",
        "def format_chat_template(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc= os.cpu_count(),\n",
        ")\n",
        "dataset = dataset.train_test_split(test_size=0.01)\n"
      ],
      "metadata": {
        "id": "ajkwWc40hL0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orpo_args = ORPOConfig(\n",
        "    learning_rate=8e-6,\n",
        "    beta=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=10,\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"./results/\",\n",
        ")\n",
        "\n",
        "trainer = ORPOTrainer(\n",
        "    model=model,\n",
        "    args=orpo_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(new_model)\n"
      ],
      "metadata": {
        "id": "P63duVluhXGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul2YLDu9CVJ7",
        "outputId": "f1741a87-72f6-4f7b-8bdc-d2894b643520"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Alishaw99/GenAI-with-Langchain.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo6JuOWZDUGe",
        "outputId": "3b2eee5e-88b5-42e5-a665-bee338aa5237"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GenAI-with-Langchain'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oshTJf0gDj7f",
        "outputId": "2500b824-0bf5-402d-ede3-cc5ccb26e524"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GenAI-with-Langchain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git remote -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK3m1OoXD4gL",
        "outputId": "d32d959f-661b-4708-dc14-24f626f639ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "origin\thttps://github.com/Alishaw99/GenAI-with-Langchain.git (fetch)\n",
            "origin\thttps://github.com/Alishaw99/GenAI-with-Langchain.git (push)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJgRbTxCD9pH",
        "outputId": "560ba8e2-9112-4fe5-c0ad-b90b63d381d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git restore --staged <file>...\" to unstage)\n",
            "\t\u001b[32mnew file:   Amanat.ipynb\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mGenAI-with-Langchain/\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git add -A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmXhlRk5EKf7",
        "outputId": "dbda9986-0dee-4184-a0e9-327167efecc8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning: adding embedded git repository: GenAI-with-Langchain\n",
            "\u001b[33mhint: You've added another git repository inside your current repository.\u001b[m\n",
            "\u001b[33mhint: Clones of the outer repository will not contain the contents of\u001b[m\n",
            "\u001b[33mhint: the embedded repository and will not know how to obtain it.\u001b[m\n",
            "\u001b[33mhint: If you meant to add a submodule, use:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit submodule add <url> GenAI-with-Langchain\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: If you added this path by mistake, you can remove it from the\u001b[m\n",
            "\u001b[33mhint: index with:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit rm --cached GenAI-with-Langchain\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: See \"git help submodule\" for more information.\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4lt1o-qHljc",
        "outputId": "ebbdb5b9-bfc8-4e3e-d9fd-b4dae6723ee1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git restore --staged <file>...\" to unstage)\n",
            "\t\u001b[32mnew file:   Amanat.ipynb\u001b[m\n",
            "\t\u001b[32mnew file:   GenAI-with-Langchain\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git commit -a -m \"first commit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et2uMSXfHwr_",
        "outputId": "71235921-84a1-4796-b152-29b96cd8b094"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@7715d8c001e8.(none)')\n"
          ]
        }
      ]
    }
  ]
}